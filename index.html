 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="Eighth Multimodal Learning and Applications Workshop ">
    <meta name="author" content="">

    <title>MULA 2025</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">8<sup>th</sup> Multimodal Learning and Applications Workshop</h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 0px 4px black, 0 0 25px black"> 
				<br></br>
				<br></br>
				<p>In conjunction with <a style="color:white" href="https://cvpr.thecvf.com/" target="_blank"><b>CVPR 2025</b></a>. </p> 
				<p>Nashville, TN </p> June 11<sup>th</sup> 2025 (Morning)
				<!-- <p>Room: West 223-224</p> -->
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">MULA 2025</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
					<li class="menuItem"><a href="#submit">Submit</a></li>
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					 <!-- <li class="menuItem"><a href="#sponsors">Sponsors</a></li>					  -->
					<li class="menuItem"><a href="#old_editions">Old Editions</a></li>
					<li class="menuItem"><a href="#contacts">Contacts</a></li>
					<!-- <li class="menuItem"><a href="#special_issue">IJCV Special Issue</a></li> -->
					
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">8<sup>th</sup> Multimodal Learning and Applications Workshop (MULA 2025)</h3>
					<!-- <div class="sub-title lead" style="text-align:center"> Best Paper Award sponsored by:</div>
                     <p class="lead"  style="text-align:center"> 
						 <a href="https://www.bosch.com/" target="_blank"><img src="img/sponsor/bosch.png" height="70"></a> 
						 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
						 <a href="https://www.snap.com/" target="_blank"><img src="img/sponsor/Snap-black.png" height="60"></a> 
					 </p>  -->

<!--
		    <p class="lead"  style="text-align:justify">  <b>NEWS! </b>Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
-->
		    </p>
                    <p class="lead"  style="text-align:justify">
						In recent years, the utilization of big data has greatly advanced Computer Vision and Machine Learning applications. However, the majority of these tasks have focused on only one modality, such as the visual one, with only a few incorporating multiple modalities like audio or thermal. Additionally, the handling of multimodal datasets remains a challenge, particularly in the areas of data acquisition, synchronization, and annotation. As a result, many research investigations have been limited to a single modality, and even when multiple modalities are considered independently, performance tends to suffer when compared to an integrated multimodal learning approach. <br/><br/>						
						Recently, there has been a growing focus on leveraging the synchronization of multimodal streams to enhance the transfer of semantic information. Various works have successfully utilized combinations such as audio/video, RGB/depth, RGB/Lidar, visual/text, text/audio, and more, achieving exceptional outcomes. Additionally, intriguing applications have emerged, employing self-supervised methodologies that enable multiple modalities to learn associations without manual labeling. This approach yields more advanced feature representations as compared to individual modality processing. Moreover, researchers have explored training paradigms that allow neural networks to perform well even when one modality is absent due to sensor failure, impaired functioning, or unfavorable environmental conditions. These topics have garnered significant interest in the computer vision community, particularly in the field of autonomous driving. Furthermore, recent attention has been directed towards the fusion of language (including Large Language models) and vision, such as in the generation of images/videos from text (e.g., DALL-E, text2video), audio (wav2clip), or vice versa (image2speech). Exploiting multimodal scenarios, diffusion models have also emerged as a fascinating framework to explore.<br/><br/>
						The information fusion from multiple sensors is a topic of major interest also in industry, the exponential growth of companies working on automotive, drone vision, surveillance or robotics are just a few examples. Many companies are trying to automate processes, by using a large variety of control signals from different sources. The aim of this workshop is to generate momentum around this topic of growing interest, and to encourage interdisciplinary interaction and collaboration between computer vision, multimedia, remote sensing, and robotics communities, that will serve as a forum for research groups from academia and industry. <br/><br/>
						We expect contributions involving, but not limited to, image, video, audio, depth, IR, IMU, laser, text, drawings, synthetic, etc. Position papers with feasibility studies and cross-modality issues with highly applicative flair are also encouraged. Multimodal data analysis is a very important bridge among vision, multimedia, remote sensing, and robotics, therefore we expect a positive response from these communities. <br/><br/>
						
						<b>Potential topics </b> include, but are not limited to: <br/><br/>
						<ul class="lead">
							<li>Multimodal learning</li>
							<li>Cross-modal learning</li>
							<li>Self-supervised learning for multimodal data</li>
							<li>Multimodal data generation and sensors</li>
							<li>Unsupervised learning on multimodal data</li>
							<li>Cross-modal adaptation</li>
							<li>Multimodal data fusion and data representation</li>
							<li>Multimodal transfer learning and Domain Adaptation</li>
							<li>Multimodal scene understanding</li>
							<li>Image and video synthesis by multimodal data</li>
							<li>Multimodal diffusion models </li>
							<li>LLM in multimodal tasks </li>
							<li>Vision and Language</li>
							<li>Vision and Sound</li>
							<li>Vision + X</li>
							<li>Multimodal biomedical analysis</li>
							<li>Multimodal applications (e.g. drone vision, autonomous driving, industrial inspection, etc.)</li>
							<li>Fairness and privacy in multimodal learning and applications</li>
						</ul>
					</p>

					

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

	<!-- Call for Papers -->
    <div id ="submit" class="content-section-b">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Submission</h3>
                    
					
                    <p class="lead"  style="text-align:justify">
						Papers will be limited to 8 pages according to the  <u><a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines" Target="blank">CVPR format</a></u> (c.f. main conference authors guidelines also for what concerns dual and double submission). 
						All papers will be reviewed by at least two reviewers with double blind policy. Papers will be selected based on relevance, significance and novelty of results, technical merit, and clarity of presentation. 
						Papers will be published in CVPR 2025 workshop proceedings. 
					</p>
					<p class="lead"  style="text-align:justify">
						All the papers should be submitted using CMT website <u><a href="https://cmt3.research.microsoft.com/MULA2025" Target="blank">https://cmt3.research.microsoft.com/MULA2025</a></u>.
					</p>
					
				</div>  

				<div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h4 class="section-heading">Important Dates</h4>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <ul class="lead">

						<!-- <del><li>Deadline for submission: March 2<sup>nd</sup>, 2025 - 23:59 Pacific Standard Time</li></del> -->
						<!-- <b>---EXTENDED---</b> -->

						<li> Deadline for submission: March 12<sup>th</sup>, 2025 - 23:59 PST (Pacific Standard Time) </li>
						<!-- <li> Deadline for submission: TBA</li> -->

						<li>Notification of acceptance  April 2<sup>nd</sup>, 2025</li>
						<!-- <del><li>Camera Ready submission deadline: April 6<sup>th</sup>, 2025</li></del>

						<b>---EXTENDED---</b> -->
						<li>Camera Ready submission (strict!) deadline: April 7<sup>th</sup>, 2025 </li> 
						<!-- <li>Camera Ready submission deadline: TBA</li> -->
						<li> Workshop date: June 11<sup>th</sup>, 2025 (Morning)</li>
				</ul>						
				</div>
				
				<!-- <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h4 class="section-heading">Camera Ready Submission instructions</h4>
                    <ul class="lead">
						<li>CVPR Workshops 2025 Camera-Ready Submission Instructions <u><a href="[long papers] CVPR Workshops 2025 Camera-Ready Submission Instructions.pdf"> Link </a> </u> </li>
						<li>Submission Site <u><a href="[long papers] CVPR Workshops 2025 Author Submission Site.pdf"> Link </a> </u></li>

				</ul>						
				</div> -->

				<p class="lead"  style="text-align:justify"></p>
				The <a href="https://cmt3.research.microsoft.com">Microsoft CMT service</a> was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.
				</p>

            </div>
        </div>
        <!-- /.container -->
    </div>
	
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program</h3>
                    <p class="lead"  style="text-align:justify;font-size:15px">
						<p class="lead"  style="text-align:justify">	
							<u>Room: 106 B</u> <br>
						</p>

			    <!-- <b> N.B.</b>  Time is CDT (Central Daylight Time); -->
			    <!-- Coming very soon! -->
			    <!--
			    <p class="lead"  style="text-align:justify">  Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
			    </p>
				-->

					<p class="lead"  style="text-align:justify">	
					08:30-08:40 - Welcome from organizers and openings remarks <br>
					</p>
					<p class="lead"  style="text-align:justify">
					08:40-09:20 - Keynote 1 - Elisa Ricci <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						<u>TITLE</u>: <strong>Toward generalizable Vision-Language Models: Improving fine-grained understanding from limited image samples and synthetic videos</strong><br>
						 <u>ABSTRACT</u>: Vision-Language Models have shown impressive performance on a wide range of tasks, 
						 yet their generalization capabilities remain a key challenge especially in fine-grained image and video understanding tasks. 
						 In this talk, I will present two recent works that explore novel strategies to address this limitation. 
						 First, I will consider the problem of few-shot adaptation for image recognition. 
						 I will introduce Two-Stage Few-Shot Adaptation (2SFS), a novel and simple strategy that explicitly separates task-level feature extraction and concept specialization. 
						 2SFS yields improved generalization capabilities over baselines and consistent gains across multiple datasets, backbones, and settings. 
						 Second, I will present SynViTA, a novel framework for improving video-language alignment using synthetic videos. 
						 SynViTA mitigates the noise and the distribution shift in generated video content by weighting samples based on semantic similarity 
						 and enforcing fine-grained caption consistency, leading to consistent gains on multiple video benchmarks and downstream tasks. 
					
					
					<p class="lead"  style="text-align:justify">
						09:20-10:00 - Keynote 2 - Shaogang Gong <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						<u>TITLE</u>: <strong>From Test-Time Inference to Small Data Generative Learning</strong><br>
						 <u>ABSTRACT</u>: Language Models (VLLM) has revolutionised machine learning in computer vision in
						 recent years largely due to their capacity for semantic reasoning in supporting visual interpretation in
						 context. Computer vision fundamentally requires answering two questions of ‘what’ and ‘where/when’.
						 However, VLLM multimodal foundation models are poor for solving the `where/when’ localisation
						 problem underpinning object detection, segmentation, video understanding, and generative synthesis
						 of details due to a lack of fine-grained domain specific knowledge without sufficient target domain
						 fine-grained training data. Moreover, increasing privacy concerns from data protection and
						 environmental concerns on energy consumption together with a need for incremental model
						 expansion in supporting decentralised and distributed user target domains of small data pose
						 fundamental challenges to the established wisdom for learning of a centralised single model from
						 exhaustive labelling. In this talk, I will present progress on exploring VLLM for test-time inference
						 without learning and for small data generative learning, using examples in automatic prompt control in
						 image segmentation by leveraging (rather than removing) VLLM hallucination for more reliable and
						 trustworthy semantic segmentation, and diffusion few-shot image generation with artifact detection to
						 overcome the limitations of LLM.
					</p>

					<p class="lead"  style="text-align:justify"> 
					10:00-10:30 - Coffee Break 
					</p>
					    
					<p class="lead"  style="text-align:justify">
					10:30-11:10 - Keynote 3 - Katerina Fragkiadaki<br>
					</p>
					<!-- <p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
					TITLE: On Video, Audio and Language Multi-Modality in Egocentric Vision <br>
					</p> -->

					<p class="lead"  style="text-align:justify"> 
						11:10-11:50 - Keynote 4 - Georgia Gkioxari
						</p>
					
					<p class="lead"  style="text-align:justify">
					11:50-12:20 - Oral Session <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
							(ID 01) - <strong>Missing Modality in Multimodal Egocentric Datasets</strong>, <i> Merey Ramazanova et al. </i> (8-min presentation + 2-min Q&A)<br> 
							(ID 06) - <strong>SplatTouch: Explicit 3D Representation Binding Vision and Touch</strong>, <i> Antonio Luigi Stefani et al. </i> (8-min presentation + 2-min Q&A)<br> 
							(ID 39) - <strong>LVP-CLIP: Revisiting CLIP for Continual Learning with Label Vector Pool</strong>, <i> Yue Ma et al. </i>  (8-min presentation + 2-min Q&A)<br>  
						</p>
	
					<p class="lead"  style="text-align:justify">
					12:20-12:30 - Closing Remarks <br>
					</p>
					
					<p class="lead"  style="text-align:justify">
					14.00-16.00 -  Poster Session  - <u> ExHall D  - poster boards #372 - #388 </u> <br>
					
					<!-- TBA -  Poster Session (all papers) <u> West Exhibit Hall - poster slots #64 - #83 </u> <br> -->
					</p>

				</div>
            </div>
            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>





	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>

				
					
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://eliricci.eu/" target="blank">Elisa Ricci </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/ericci.jpeg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p> Elisa Ricci is a Professor at the Department of Information Engineering and Computer Science (DISI) at University of Trento and the Head of the Research Unit Deep Visual Learning at Fondazione Bruno Kessler. Elisa is also the Coordinator of Doctoral Program in Information Engineering and Computer Science at University of Trento. She is an ELLIS and a IAPR Fellow.
							Her research lies at the intersection of computer vision, deep learning and robotics perception. She is interested in developing novel approaches for learning from visual and multi-modal data in an open world, with particular emphasis in methods for domain adaptation, continual and self-supervised learning.
						</p>
				</div>
				</div>

				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://gkioxari.github.io/" target="blank"> Georgia Gkioxari</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/gkioxari.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Georgia Gkioxari is an Assistant Professor of Computing + Mathematical Sciences at Caltech and a William H. Hurt scholar. 
							She is also a visiting researcher at Meta AI in the Embodied AI team. 
							From 2016 to 2022, she was a research scientist at Meta's FAIR team. 
							She received my PhD from UC Berkeley, where she was advised by Jitendra Malik. 
							She did her bachelors in ECE at NTUA in Athens, Greece, where she worked with Petros Maragos.
							She is the recipient of the PAMI Young Researcher Award (2021). 
						</p>
					</div>
				</div>	

				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.eecs.qmul.ac.uk/~sgg/" target="blank"> Shaogang Gong </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/SG.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p> 
							Sean Gong pioneered person re-identification and video behaviour analysis
							for law enforcement. Prof Gong is elected a Fellow of the Royal Academy of
							Engineering, and served on the steering panel of the UK government Chief
							Scientific Adviser’s Science Review on Security. He has made unique
							contributions to the engineering of AI video analytics for law enforcement and
							the security industry and was awarded an Institution for Engineering and
							Technology Achievement Medal for Vision Engineering for outstanding
							achievement and superior performance in contributing to public safety. A
							commercial system built on his research won an Aerospace Defence Security Innovation Award, and
							a Global Frost & Sullivan Award for Technical Innovation for Law Enforcement Video Forensics
							Technology. Gong is Professor of Visual Computation and Director of the Computer Vision Laboratory
							at Queen Mary University of London, a Turing Fellow of the Alan Turing Institute, a member of the UK
							Computing Research Committee. He founded Vision Semantics and served as the Chief Scientist of
							three start-ups. He is a Distinguished Scientist of Veritone. He received his DPhil from Oxford.
						</p>
					</div>
				</div>	

				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.cs.cmu.edu/~katef/" target="blank"> Katerina Fragkiadaki</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/Fragkiadaki.png" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Katerina Fragkiadaki is a JPMorgan Chase Associate Professor of Computer Science in the Machine Learning Department at Carnegie Mellon University. 
							She works in Artificial Intelligence at the intersection of Computer Vision, Machine Learning, Language Understanding and Robotics. 
							Prior to joining MLD's faculty she spent three years as a post doctoral researcher first at UC Berkeley working with Jitendra Malik and then at Google Research in Mountain View working with the video group. 
							She completed her Ph.D. in GRASP, UPenn with Jianbo Shi . 
							She did her undergraduate studies at the National Technical University of Athens and before that she was in Crete. 
						</p>
					</div>
				</div>	

				<!-- <div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://mancinimassimiliano.github.io/" target="blank">Massimiliano Mancini</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/mancini.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Massimiliano Mancini is an ELLIS member and an Assistant Professor at the University of Trento. 
							He completed his Ph.D. at the Sapienza University of Rome, co-advised by Barbara Caputo and Elisa Ricci. 
							During his Ph.D., he was part of the TeV lab at Fondazione Bruno Kessler, the VANDAL lab at the Italian Institute of Technology, and a visiting student at the KTH Royal Institute of Technology. 
							After his Ph.D., he joined the University of Tübingen as a postdoc in the Explainable Machine Learning group led by Zeynep Akata. 
							He serves as area chair for major conferences in the field (CVPR, ECCV, NeurIPS, ICRA) and as an associate/area editor for CVIU and TMLR. 
							His research focuses on efficient transfer learning, cross-domain generalization, continual learning, automatic bias identification, and compositional reasoning.
						</p>
					</div> 
				</div>	 -->

           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
		        <div class="row wow fadeInRightBig"  data-animation-delay="200">
				<h3 class="section-heading">Organizers</h3>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="">   
					<h4 class="section-heading"><center>Pietro Morerio</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/morerio.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia, Italy</i></center></h5>
				</div>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Paolo Rota</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rota.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Università di Trento, Italy</i></center></h5>
				</div>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Michael Ying Yang</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/yang.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Bath, UK</i></center></h5>
				</div> 
				
				<!-- <div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Massimiliano Mancini</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/mancini.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div> -->
				
				</div>
				
				<!-- <div class="row wow fadeInRightBig"  data-animation-delay="200"> -->
				<!-- <div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Zeynep Akata</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/akata.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div> -->
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Bodo Rosenhahn</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rosenhahn.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Institut für Informationsverarbeitung, Leibniz-Universität Hannover, Germany</i></center></h5>
				</div>


				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Vittorio Murino</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/murino.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia & Università di Verona, Italy </i></center></h5>
				</div>
			</div>
        </div>
    </div>

	<!-- Program Committee -->
    <div id ="committee" class="content-section-b" style="border-top: 0">
        <div class="container">			
            <div class="row">			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"> Acknowledgments</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
					<p class="lead"  style="text-align:justify">
						We gratefully acknowledge our reviewers

						<ul class="lead">
							<!-- AJ	Piergiovanni,
							Alina	Roitberg,
							Andrea	Pilzer,
							Andrea	Zunino,
							Anelia	Angelova,
							Anil Osman	Tur,
							Arif	Mahmood,
							Carles	Ventura,
							Christoph	Reinders,
							Dario	Fontanel,
							Davide	Talon,
							Dayan	Guan,
							Enrico	Fini,
							Fabio	Cermelli,
							Giacomo	Zara,
							Gianluca	Scarpellini,
							Guanglei	Yang,
							Haidong	Zhu,
							Haoyu	Dong,
							Hari Prasanna	Das ,
							Ichraf	Lahouli,
							Jae Myung	Kim,
							Jiguo	Li,
							Karsten	Roth,
							Leonard	Salewski,
							Letitia	Parcalabescu,
							Limin	Wang,
							Mihee	Lee,
							Nicola	Dall'Asen,
							Praneet	Dutta,
							Pritish	Sahu,
							Qing	Wan,
							Rico	Jonschkowski,
							Ruggero	Ragonesi,
							Sharath	Koorathota,
							Shih-Han	Chou,
							Shyamgopal	Karthik,
							Suvarna	Kadam,
							Tal	Hakim,
							Thiago	Oliveira-Santos,
							Thomas	Hummel,
							Thomas	Theodoridis,
							Uddeshya	Upadhyay,
							Victor	Turrisi da Costa,
							Vladimir	Kniaz,
							Vladimir	Pavlovic,
							Wentong	Liao,
							Woojeong	Jin,
							Xu	Dong,
							Yanbei	Chen,
							Yoonsuck	Choe,
							Yue	Song,
							Ze	Wang. -->

						</ul> 

					</p>

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

    <!-- Sponsor -->
     <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> 
         <div class="container">			 
             <div class="row">			 
				<div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div				
                 <div class="wow fadeInRightBig" data-animation-delay="200">    
                     <h3 class="section-heading"> Sponsors </h3> 
					<div class="sub-title lead">We gratefully acknowlegde our sponsors for supporting the Best Paper Award</div>
					<p> </p>
                     <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.snap.com/" target="_blank"><img src="img/sponsor/Snap-black.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.snap.com/" role="button">Visit Website</a></p
					<p> &nbsp;&nbsp;</p>
					 <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.bosch.com/" target="_blank"><img src="img/sponsor/bosch.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.bosch.com/" role="button">Visit Website</a></p
				 </div>    
             </div> 
         </div> 
     </div>  -->

	
	
<!-- <div id ="special_issue" class="content-section-a" style="border-top: 0">
        <div class="container">						
                <div class="wow fadeInRightBig" data-animation-delay="200">   
					<h3 class="section-heading"> IJCV Special Issue </h3>
					
					<a href="https://www.springer.com/journal/11263/updates/23159852?gclid=Cj0KCQjwhqaVBhCxARIsAHK1tiNlJcecKyxOzacWAGubPisKaHP7P6XkYYGt1xXKnuxPG-rTPP2FjV4aAmdAEALw_wcB" target="_blank"><img src="img/special_issue.png" width="1000"></a> 

					
		</div>
    </div>
     -->
<!--
     Old editions 
-->
    <div id ="old_editions" class="content-section-a" style="border-top: 0">
        <div class="container">						
                <div class="wow fadeInRightBig" data-animation-delay="200">   
					<h3 class="section-heading"> Old Editions </h3>
					
					<ul class="lead">
						<li> <b>1<sup>st</sup> edition @ ECCV 2018 - Munich, Germany</b>, <a href="https://mula2018.github.io/">Link</a> </li>
						<li> <b>2<sup>nd</sup> edition @ CVPR 2019 - Long Beach</b>,  <a href="index_2019.html"> Link </a></li>
						<li> <b>3<sup>rd</sup> edition @ CVPR 2020 - VIRTUAL</b>,  <a href="https://mul-workshop.github.io/">Link</a>  </li>
						<li> <b>4<sup>th</sup> edition @ CVPR 2021 - VIRTUAL</b>,  <a href="index_2021.html"> Link </a></li>
						<li> <b>5<sup>th</sup> edition @ CVPR 2022 - New Orleans</b>,  <a href="index_2022.html"> Link </a></li>
						<li> <b>6<sup>th</sup> edition @ CVPR 2023 - Vancouver</b>,  <a href="index_2023.html"> Link </a></li>
						<li> <b>7<sup>th</sup> edition @ CVPR 2024 - Seattle</b>,  <a href="index_2024.html"> Link </a></li>
						
					</ul>
		</div>
    </div>


	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						For additional info please contact us <u><a style="color:white" href="mailto:mula.workshop@gmail.com">here</a></u> 
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>
		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
