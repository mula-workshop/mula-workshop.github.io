 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="Fourth Multimodal Learning and Applications Workshop ">
    <meta name="author" content="">

    <title>MULA 2024</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">7<sup>th</sup> Multimodal Learning and Applications Workshop</h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 0px 4px black, 0 0 25px black"> 
				<br></br>
				<br></br>
				<p>In conjunction with <a style="color:white" href="https://cvpr.thecvf.com/" target="_blank"><b>CVPR 2024</b></a>. </p> 
				<p>Seattle, WA </p> June 18<sup>th</sup> 2024 (Full day)
				<!-- <p>Room: West 223-224</p> -->
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">MULA 2024</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
					<li class="menuItem"><a href="#submit">Submit</a></li>
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					 <!-- <li class="menuItem"><a href="#sponsors">Sponsors</a></li>					  -->
					<li class="menuItem"><a href="#old_editions">Old Editions</a></li>
					<li class="menuItem"><a href="#contacts">Contacts</a></li>
					<!-- <li class="menuItem"><a href="#special_issue">IJCV Special Issue</a></li> -->
					
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">7<sup>th</sup> Multimodal Learning and Applications Workshop (MULA 2024)</h3>
					<!-- <div class="sub-title lead" style="text-align:center"> Best Paper Award sponsored by:</div>
                     <p class="lead"  style="text-align:center"> 
						 <a href="https://www.bosch.com/" target="_blank"><img src="img/sponsor/bosch.png" height="70"></a> 
						 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
						 <a href="https://www.snap.com/" target="_blank"><img src="img/sponsor/Snap-black.png" height="60"></a> 
					 </p>  -->

<!--
		    <p class="lead"  style="text-align:justify">  <b>NEWS! </b>Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
-->
		    </p>
                    <p class="lead"  style="text-align:justify">
						In recent years, the utilization of big data has greatly advanced Computer Vision and Machine Learning applications. However, the majority of these tasks have focused on only one modality, such as the visual one, with only a few incorporating multiple modalities like audio or thermal. Additionally, the handling of multimodal datasets remains a challenge, particularly in the areas of data acquisition, synchronization, and annotation. As a result, many research investigations have been limited to a single modality, and even when multiple modalities are considered independently, performance tends to suffer when compared to an integrated multimodal learning approach. <br/><br/>						
						Recently, there has been a growing focus on leveraging the synchronization of multimodal streams to enhance the transfer of semantic information. Various works have successfully utilized combinations such as audio/video, RGB/depth, RGB/Lidar, visual/text, text/audio, and more, achieving exceptional outcomes. Additionally, intriguing applications have emerged, employing self-supervised methodologies that enable multiple modalities to learn associations without manual labeling. This approach yields more advanced feature representations as compared to individual modality processing. Moreover, researchers have explored training paradigms that allow neural networks to perform well even when one modality is absent due to sensor failure, impaired functioning, or unfavorable environmental conditions. These topics have garnered significant interest in the computer vision community, particularly in the field of autonomous driving. Furthermore, recent attention has been directed towards the fusion of language (including Large Language models) and vision, such as in the generation of images/videos from text (e.g., DALL-E, text2video), audio (wav2clip), or vice versa (image2speech). Exploiting multimodal scenarios, diffusion models have also emerged as a fascinating framework to explore.<br/><br/>
						The information fusion from multiple sensors is a topic of major interest also in industry, the exponential growth of companies working on automotive, drone vision, surveillance or robotics are just a few examples. Many companies are trying to automate processes, by using a large variety of control signals from different sources. The aim of this workshop is to generate momentum around this topic of growing interest, and to encourage interdisciplinary interaction and collaboration between computer vision, multimedia, remote sensing, and robotics communities, that will serve as a forum for research groups from academia and industry. <br/><br/>
						We expect contributions involving, but not limited to, image, video, audio, depth, IR, IMU, laser, text, drawings, synthetic, etc. Position papers with feasibility studies and cross-modality issues with highly applicative flair are also encouraged. Multimodal data analysis is a very important bridge among vision, multimedia, remote sensing, and robotics, therefore we expect a positive response from these communities. <br/><br/>
						
						<b>Potential topics </b> include, but are not limited to: <br/><br/>
						<ul class="lead">
							<li>Multimodal learning</li>
							<li>Cross-modal learning</li>
							<li>Self-supervised learning for multimodal data</li>
							<li>Multimodal data generation and sensors</li>
							<li>Unsupervised learning on multimodal data</li>
							<li>Cross-modal adaptation</li>
							<li>Multimodal data fusion and data representation</li>
							<li>Multimodal transfer learning and Domain Adaptation</li>
							<li>Multimodal scene understanding</li>
							<li>Image and video synthesis by multimodal data</li>
							<li>Multimodal diffusion models </li>
							<li>LLM in multimodal tasks </li>
							<li>Vision and Language</li>
							<li>Vision and Sound</li>
							<li>Vision + X</li>
							<li>Multimodal biomedical analysis</li>
							<li>Multimodal applications (e.g. drone vision, autonomous driving, industrial inspection, etc.)</li>
							<li>Fairness and privacy in multimodal learning and applications</li>
						</ul>
					</p>

					

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

	<!-- Call for Papers -->
    <div id ="submit" class="content-section-b">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Submission</h3>
                    
					
                    <p class="lead"  style="text-align:justify">
						Papers will be limited to 8 pages according to the  <u><a href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines" Target="blank">CVPR format</a></u> (c.f. main conference authors guidelines). 
						All papers will be reviewed by at least two reviewers with double blind policy. Papers will be selected based on relevance, significance and novelty of results, technical merit, and clarity of presentation. 
						Papers will be published in CVPR 2024 workshop proceedings. 
					</p>
					<p class="lead"  style="text-align:justify">
						All the papers should be submitted using CMT website <u><a href="https://cmt3.research.microsoft.com/MULA2024" Target="blank">https://cmt3.research.microsoft.com/MULA2024</a></u>.
					</p>

				</div>  

				<div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h4 class="section-heading">Important Dates</h4>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <ul class="lead">

						<!-- <del><li>Deadline for submission: March 9<sup>th</sup>, 2024 - 23:59 Pacific Standard Time</li></del>
						<b>---EXTENDED---</b> -->

						<li> Deadline for submission: March 2<sup>nd</sup>, 2024 - 23:59 Pacific Standard Time </li>
						<!-- <li> Deadline for submission: TBA</li> -->

						<li>Notification of acceptance  April 7<sup>th</sup>, 2024</li>
						<!-- <del><li>Camera Ready submission deadline: April 6<sup>th</sup>, 2024</li></del>

						<b>---EXTENDED---</b> -->
						<li>Camera Ready submission deadline: April 11<sup>th</sup>, 2024</li>
						<!-- <li>Camera Ready submission deadline: TBA</li> -->

						<li> Workshop date: June 18<sup>th</sup>, 2024 (Full day)</li>

				</ul>						
				</div>  				
            </div>
        </div>
        <!-- /.container -->
    </div>
	
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program</h3>
                    <p class="lead"  style="text-align:justify;font-size:15px">
						<p class="lead"  style="text-align:justify">	
							<!-- <u>Room: West 223-224</u> <br> -->
						</p>
                    	<!-- Room: Seaside 7 -->

			    <!-- <b> N.B.</b>  Time is CDT (Central Daylight Time); -->
			    TBD
			    
<!--
			    <p class="lead"  style="text-align:justify">  Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
			    </p>
-->

					<!-- <p class="lead"  style="text-align:justify">	
					09:15-09:30 - Welcome from organizers and openings remarks <br>
					</p>
					<p class="lead"  style="text-align:justify">
					09:30-10:15 - Keynote 1 - <b> Nicu Sebe </b>  (University of Trento) <br>
					</p> -->
					<!-- <p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
					  <i> Abstract: </i>  The tremendous growth of multimodal video data in recent years has increased the demand for efficient multimodal deep neural network models, particularly in domains where real-time inference is essential. While significant progress has been made on model compression and acceleration for video understanding, most existing methods rely on one-size-fits-all models, which apply the same amount of computation for all video segments across all modalities. In this talk, I will instead cover methods that adaptively change computation depending on the content of the input. In particular, in the context of audio-visual action recognition, I will describe a method that adaptively decides which modality to use for each video segment (deciding where to look at and listen to in the video), with the goal of improving both accuracy and efficiency. Finally, I will conclude my talk by describing ongoing work that integrates this technology into a system for auto-curation of sports highlights based on multimodal video understanding. <br>
					</p> -->
<!-- 
					<p class="lead"  style="text-align:justify">
						10:15-10:45 - Coffee break <br>
						</p>

					<p class="lead"  style="text-align:justify"> 
					10:45-11:30 - Oral Session 1 (10-min presentations + 5-min Q&A)
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						(ID 2) - TFRGAN: Leveraging Text Information for Blind Face Restoration with Extreme Degradation - <i>  Chengxing Xie (Xidian); Qian Ning (Xidian University); Weisheng Dong (Xidian University)*; Guangming Shi (Xidian University) </i> <br> 
						(ID 3) - The MONET dataset: Multimodal drone thermal dataset recorded in rural scenarios - <i> Luigi Riz (Fondazione Bruno Kessler); Andrea Caraffa (Fondazione Bruno Kessler); Matteo Bortolon (Fondazione Bruno Kessler;Istituto Italiano di Tecnologia (IIT);University of Trento); Mohamed Lamine Mekhalfi (Fondazione Bruno Kessler); Davide Boscaini (Fondazione Bruno Kessler); André F. Moura (INESC TEC); José  Filipe Antunes (INESC TEC); André M. Dias (INESC TEC); Hugo  M Silva (INESCTEC); Andreas  Leonidou (The Cyprus Institute ); Christos Constantinides (CARE-C); Christos M Keleshis (The Cyprus Institute); Dante  Abate (The Cyprus Institute); Fabio Poiesi (Fondazione Bruno Kessler)* </i> <br>
						(ID 4) - SSGVS: Semantic  Scene Graph-to-Video Synthesis - <i> Yuren Cong (Leibniz University Hannover); Jinhui Yi (University of Bonn); Bodo Rosenhahn (Leibniz University Hannover); Michael Ying Yang (University of Twente)*</i> <br>
					</p>
					    
					<p class="lead"  style="text-align:justify">
					11:30-12:15 - Keynote 2 - <b> Helge Rhodin </b> (University of British Columbia) <br>
					</p>

					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
					TITLE: "Unpaired Multi-modal Learning" <br>
					ABSTRACT: Multi-modal learning becomes easy when paired data is available. However, capturing multiple modalities simultaneously can be tricky or even impossible. For example, capturing 3D human motion trajectories is best done in a studio, while the corresponding video should be captured outdoors to achieve realism. In this presentation, I will showcase various new and established scenarios I encountered, ranging from the creation of a new sign language (video + audio) to neuroscience research on mice (video + brain neural activation), as well as human motion analysis (video + pose). All of these scenarios can be reduced to a form of correspondence finding. It is not the classical image correspondence, but rather finding correspondences across domains---a challenging problem with immense untapped potential.
					</p>
					
					<p class="lead"  style="text-align:justify">
					12:15-13:15 - Lunch <br>
					</p>

					<p class="lead"  style="text-align:justify">
					13:15-14:00 - Keynote 3 - <b> Alireza Fathi </b> (Google Research) <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
					TITLE: "Retrieval and Tool Augmented Visual Language Models" <br>
					</p>
					
					<p class="lead"  style="text-align:justify">
					14:00-14:45 - Oral Session 2  (10-min presentations + 5-min Q&A)
					</p>

					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						(ID 5) - Multi Event Localization by Audio-Visual Fusion with Omnidirectional Camera and Microphone Array - <i> Wenru Zheng (Tokyo Institute of Technology)*; Ryota Yoshihashi (Tokyo Institute of Technology); Rei Kawakami (Tokyo Institute of Technology); Ikuro Sato (Tokyo Institute of Technology / Denso IT Laboratory); Asako Kanezaki (Tokyo Institute of Technology) </i> <br> 
				   		(ID 8) - Dynamic Multimodal Fusion - <i> Zihui Xue (The University of Texas at Austin)*; Radu Marculescu (The University of Texas at Austin) </i> <br>
				   		(ID 9) - Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval - <i> Jae Myung Kim (University of Tuebingen)*; A. Sophia Koepke (University of Tübingen); Cordelia Schmid (Inria/Google); Zeynep Akata (University of Tübingen) </i> <br>
					</p>

					<p class="lead"  style="text-align:justify">
					14:45-15:15 - Coffee break <br>
					</p>

					<p class="lead"  style="text-align:justify">
					15:15-16:00 - Oral Session 3  (10-min presentations + 5-min Q&A)
					</p>

					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						(ID 10) - Adapting Grounded Visual Question Answering Models to Low Resource Languages - <i> Ying Wang (New York University)*; Jonas Pfeiffer (Google Research ); Nicolas Carion (NYU); Yann LeCun (New York University); Aishwarya Kamath (New York University) </i> <br> 
				   		(ID 11) - SEM-POS: Grammatically and Semantically Correct Video Captioning - <i> Asmar Nadeem (University of Surrey)*; Adrian Hilton (University of  Surrey); Robert Dawes (BBC Research); Graham Thomas (BBC); Armin Mustafa (University of Surrey) </i> <br>
				   		(ID 15) - Robust Multiview Multimodal Driver Monitoring System Using Masked Multi-Head Self-Attention - <i> Yiming Ma (University of Warwick)*; Victor Sanchez (University of Warwick); Soodeh SN Nikan (Ford Motor Company); Devesh Upadhyay (Ford Motor Co.); Bhushan S Atote (University of Warwick); Tanaya Guha (University of Glasgow) </i> <br>
					</p>

					<p class="lead"  style="text-align:justify">
					16:00-16:45 - Keynote 4 - <b> Aleksander Hołyński </b> ((UC Berkeley (BAIR) and Google Research)) <br> 
					</p>
					<p class="lead"  style="text-align:justify">
					16:45-16:50 - Closing Remarks <br>
					</p>
					
					<p class="lead"  style="text-align:justify">
					16:50-18:00 -  Poster Session (all papers) <u> West Exhibit Hall - poster slots #64 - #83 </u> <br>
					</p> -->

				</div>
            </div>
            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>





	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>

				
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://dimadamen.github.io/" target="blank">Dima Damen </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/damen.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p> Dima Damen is a Professor of Computer Vision at the University of Bristol and Senior Research Scientist at Google DeepMind. 
							Dima is currently an EPSRC Fellow (2020-2025), focusing her research interests in the automatic understanding of object interactions, 
							actions and activities using wearable visual (and depth) sensors. She is best known for her leading works in Egocentric Vision, 
							and has also contributed to novel research questions including mono-to-3D, video object segmentation, assessing action completion, 
							domain adaptation, skill/expertise determination from video sequences, discovering task-relevant objects, dual-domain and dual-time learning 
							as well as multi-modal fusion using vision, audio and language.
						</p>
				</div>
				</div>

				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="blank">Christian Theobalt</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/theobalt.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p> Christian Theobalt is a professor of computer science and the director of the department “Visual Computing and Artificial Intelligence” 
							with the Max Planck Institute for Informatics, Germany. He is also a professor with Saarland University. His research lies on the boundary between 
							Computer Vision and Computer Graphics. He received several awards, for instance the Otto Hahn Medal of the Max Planck Society (2007), 
							the EUROGRAPHICS Young Researcher Award (2009), the German Pattern Recognition Award (2012), an ERC Starting Grant (2013), 
							an ERC Consolidator Grant (2017), and the Eurographics Outstanding Technical Contributions Award (2020). 
							In 2015, he was elected one of Germany's top 40 innovators under 40 by the magazine Capital.
						</p>
					</div> 
				</div>		
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://gkioxari.github.io/" target="blank">Georgia Gkioxari</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/gkioxari.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Georgia Gkioxari is Assistant Professor of Computing + Mathematical Sciences at Caltech and a William H. Hurt scholar. 
							From 2016 to 2022, she was a research scientist at Meta, at FAIR. She received her PhD from UC Berkeley, where she was advised by Jitendra Malik. 
							She did her bachelors in ECE at NTUA in Athens, Greece, where she worked with Petros Maragos.
							She is the recipient of the PAMI Young Researcher Award (2021) and received the PAMI Mark Everingham Award (2021) for the Detectron Library Suite. 
							She was was named one of 30 influential women advancing AI in 2019 by ReWork and was nominated for the Women in AI Awards in 2020 by VentureBeat. 
						</p>
					</div> 
				</div>	
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://research.nvidia.com/labs/dvl/author/laura-leal-taixe/" target="blank"> Laura Leal-Taixé</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/taixe.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Laura Leal-Taixé is a Senior Research Manager at NVIDIA and also an Adjunct Professor at the Technical University of Munich (TUM), 
							leading the Dynamic Vision and Learning group. From 2018 until 2022, she was a tenure-track professor at TUM. Before that, she spent two years as a 
							postdoctoral researcher at ETH Zurich, Switzerland, and a year as a senior postdoctoral researcher in the Computer Vision Group at the Technical University in Munich. 
							She obtained her PhD from the Leibniz University of Hannover in Germany, spending a year as a visiting scholar at the University of Michigan, 
							Ann Arbor, USA. She pursued B.Sc. and M.Sc. in Telecommunications Engineering at the Technical University of Catalonia (UPC) in her native city of Barcelona. 
							She went to Boston, USA to do her Masters Thesis at Northeastern University with a fellowship from the Vodafone foundation. 
							She is a recipient of the Sofja Kovalevskaja Award of 1.65 million euros in 2017, the Google Faculty Award in 2021, and the ERC Starting Grant in 2022.
						</p>
					</div>
				</div>	

				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.ceessnoek.info/" target="blank"> Cees Snoek</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/snoek.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Video & Image Sense Lab. 
							He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence. 
							At University spin-off Kepler Vision Technologies he acts as Chief Scientific Officer. 
							Professor Snoek is also the director of the ELLIS Amsterdam Unit and scientific director of Amsterdam AI, a collaboration between government, academic, 
							medical and other organisations in Amsterdam to help the city develop and deploy responsible AI.
							He received the M.Sc. degree in business information systems (2000) and the Ph.D. degree in computer science (2005) both from the University of Amsterdam, The Netherlands. 
						</p>
					</div>
				</div>	

				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="http://imagine.enpc.fr/~varolg/" target="blank"> Gül Varol</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/varol.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify;padding: 70px 0" data-animation-delay="200">   
						<p>  
							Gül Varol is a permanent researcher (~Assist. Prof.) in the IMAGINE team at École des Ponts ParisTech. 
							Previously, she was a postdoctoral researcher at the University of Oxford (VGG), working with Andrew Zisserman. 
							She obtained her PhD from the WILLOW team of Inria Paris and École Normale Supérieure (ENS). 
							Her thesis, co-advised by Ivan Laptev and Cordelia Schmid, received the PhD awards from ELLIS and AFRIF. 
							During her PhD, she spent time at MPI, Adobe, and Google. Prior to that, she received her BS and MS degrees from Boğaziçi University. 
							She regularly serves as an Area Chair at major computer vision conferences, and will serve as a Program Chair at ECCV'24. 
							She has co-organized a number of workshops at CVPR, ICCV, ECCV, and NeurIPS. Her research interests cover vision and language applications, 
							including video representation learning, human motion synthesis, and sign languages.
						</p>
					</div>
				</div>	


           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
		        <div class="row wow fadeInRightBig"  data-animation-delay="200">
				<h3 class="section-heading">Organizers</h3>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Paolo Rota</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rota.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Università di Trento, Italy</i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="">   
					<h4 class="section-heading"><center>Pietro Morerio</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/morerio.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia, Italy</i></center></h5>
				</div>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Michael Ying Yang</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/yang.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Bath, UK</i></center></h5>
				</div> 
				
				<!-- <div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Massimiliano Mancini</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/mancini.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div> -->
				
				</div>
				
				<!-- <div class="row wow fadeInRightBig"  data-animation-delay="200"> -->
				<!-- <div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Zeynep Akata</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/akata.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div> -->
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Bodo Rosenhahn</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rosenhahn.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Institut für Informationsverarbeitung, Leibniz-Universität Hannover, Germany</i></center></h5>
				</div>


				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Vittorio Murino</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/murino.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia & Università di Verona, Italy </i></center></h5>
				</div>
			</div>
        </div>
    </div>

	<!-- Program Committee -->
    <div id ="committee" class="content-section-b" style="border-top: 0">
        <div class="container">			
            <div class="row">			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"> Acknowledgments</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
					<p class="lead"  style="text-align:justify">
						We gratefully acknowledge our reviewers

						<ul class="lead">
							<!-- AJ	Piergiovanni,
							Alina	Roitberg,
							Andrea	Pilzer,
							Andrea	Zunino,
							Anelia	Angelova,
							Anil Osman	Tur,
							Arif	Mahmood,
							Carles	Ventura,
							Christoph	Reinders,
							Dario	Fontanel,
							Davide	Talon,
							Dayan	Guan,
							Enrico	Fini,
							Fabio	Cermelli,
							Giacomo	Zara,
							Gianluca	Scarpellini,
							Guanglei	Yang,
							Haidong	Zhu,
							Haoyu	Dong,
							Hari Prasanna	Das ,
							Ichraf	Lahouli,
							Jae Myung	Kim,
							Jiguo	Li,
							Karsten	Roth,
							Leonard	Salewski,
							Letitia	Parcalabescu,
							Limin	Wang,
							Mihee	Lee,
							Nicola	Dall'Asen,
							Praneet	Dutta,
							Pritish	Sahu,
							Qing	Wan,
							Rico	Jonschkowski,
							Ruggero	Ragonesi,
							Sharath	Koorathota,
							Shih-Han	Chou,
							Shyamgopal	Karthik,
							Suvarna	Kadam,
							Tal	Hakim,
							Thiago	Oliveira-Santos,
							Thomas	Hummel,
							Thomas	Theodoridis,
							Uddeshya	Upadhyay,
							Victor	Turrisi da Costa,
							Vladimir	Kniaz,
							Vladimir	Pavlovic,
							Wentong	Liao,
							Woojeong	Jin,
							Xu	Dong,
							Yanbei	Chen,
							Yoonsuck	Choe,
							Yue	Song,
							Ze	Wang. -->

						</ul> 

					</p>

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

    <!-- Sponsor -->
     <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> 
         <div class="container">			 
             <div class="row">			 
				<div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div				
                 <div class="wow fadeInRightBig" data-animation-delay="200">    
                     <h3 class="section-heading"> Sponsors </h3> 
					<div class="sub-title lead">We gratefully acknowlegde our sponsors for supporting the Best Paper Award</div>
					<p> </p>
                     <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.snap.com/" target="_blank"><img src="img/sponsor/Snap-black.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.snap.com/" role="button">Visit Website</a></p
					<p> &nbsp;&nbsp;</p>
					 <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.bosch.com/" target="_blank"><img src="img/sponsor/bosch.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.bosch.com/" role="button">Visit Website</a></p
				 </div>    
             </div> 
         </div> 
     </div>  -->

	
	
<!-- <div id ="special_issue" class="content-section-a" style="border-top: 0">
        <div class="container">						
                <div class="wow fadeInRightBig" data-animation-delay="200">   
					<h3 class="section-heading"> IJCV Special Issue </h3>
					
					<a href="https://www.springer.com/journal/11263/updates/23159852?gclid=Cj0KCQjwhqaVBhCxARIsAHK1tiNlJcecKyxOzacWAGubPisKaHP7P6XkYYGt1xXKnuxPG-rTPP2FjV4aAmdAEALw_wcB" target="_blank"><img src="img/special_issue.png" width="1000"></a> 

					
		</div>
    </div>
     -->
<!--
     Old editions 
-->
    <div id ="old_editions" class="content-section-a" style="border-top: 0">
        <div class="container">						
                <div class="wow fadeInRightBig" data-animation-delay="200">   
					<h3 class="section-heading"> Old Editions </h3>
					
					<ul class="lead">
						<li> <b>1<sup>st</sup> edition @ ECCV 2018 - Munich, Germany</b>, <a href="https://mula2018.github.io/">Link</a> </li>
						<li> <b>2<sup>nd</sup> edition @ CVPR 2019 - Long Beach</b>,  <a href="index_2019.html"> Link </a></li>
						<li> <b>3<sup>rd</sup> edition @ CVPR 2020 - VIRTUAL</b>,  <a href="https://mul-workshop.github.io/">Link</a>  </li>
						<li> <b>4<sup>th</sup> edition @ CVPR 2021 - VIRTUAL</b>,  <a href="index_2021.html"> Link </a></li>
						<li> <b>5<sup>th</sup> edition @ CVPR 2022 - New Orleans</b>,  <a href="index_2022.html"> Link </a></li>
						<li> <b>5<sup>th</sup> edition @ CVPR 2023 - Vancouver</b>,  <a href="index_2023.html"> Link </a></li>
						
					</ul>
		</div>
    </div>


	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						For additional info please contact us <u><a style="color:white" href="mailto:mula.workshop@gmail.com">here</a></u> 
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>
		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
